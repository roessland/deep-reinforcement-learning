{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1182204",
   "metadata": {},
   "source": [
    "# Deep RL Bootcamp Lab 1\n",
    "\n",
    "https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244043c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "print(env.__doc__)\n",
    "\n",
    "\n",
    "ACTION_UP = 3\n",
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 2\n",
    "ACTION_DOWN = 1\n",
    "\n",
    "action_name = {\n",
    "    ACTION_UP: 'up',\n",
    "    ACTION_LEFT: 'left',\n",
    "    ACTION_RIGHT: 'right',\n",
    "    ACTION_DOWN: 'down'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c05529",
   "metadata": {},
   "source": [
    "### Play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c96411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gymnasium.utils.play import play\n",
    "import numpy as np\n",
    "\n",
    "mapping = {\n",
    "    \"w\": ACTION_UP,\n",
    "    \"a\": ACTION_LEFT,\n",
    "    \"s\": ACTION_DOWN,\n",
    "    \"d\": ACTION_RIGHT,\n",
    "}\n",
    "\n",
    "#play(env, keys_to_action=mapping, noop=2, fps=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ed7b2",
   "metadata": {},
   "source": [
    "## Step through the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def render_inline(env):\n",
    "    img = env.render()\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def simulate_random_episode():\n",
    "    env.reset()\n",
    "    for t in range(100):\n",
    "        render_inline(env)\n",
    "        action = env.action_space.sample()\n",
    "        print(f\"Going {action_name[action]}\")\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated:\n",
    "            break\n",
    "    assert terminated\n",
    "    render_inline(env)\n",
    "\n",
    "simulate_random_episode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf464f",
   "metadata": {},
   "source": [
    "## Making a Markov Decision Process from the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ac32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    def __init__(self, P, num_states, num_actions, desc=None):\n",
    "        self.P = P\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.desc = desc\n",
    "    \n",
    "    def probability(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        Probability of going to `next_state` if taking `action` in `state`.\n",
    "        \"\"\"\n",
    "        return sum(tup[0] for tup in self.P[state][action] if tup[1] == next_state)\n",
    "    \n",
    "    def reward(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        Reward from going from `state` to `next_state` via `action`.\n",
    "        \"\"\"\n",
    "        return sum(tup[2] for tup in self.P[state][action] if tup[1] == next_state)\n",
    "\n",
    "\n",
    "#state = 2\n",
    "#action = ACTION_DOWN\n",
    "#env.P[state][action]\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "mdp = MDP(env.P, num_states, num_actions)\n",
    "\n",
    "mdp.reward(14, ACTION_RIGHT, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914f934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c62e248",
   "metadata": {},
   "source": [
    "```\n",
    "env.P[14][ACTION_RIGHT] = \n",
    "[(0.3333333333333333, 14, 0.0, False),\n",
    " (0.3333333333333333, 15, 1.0, True),\n",
    " (0.3333333333333333, 10, 0.0, False)]\n",
    "```\n",
    "Which means the probability of reaching the goal at 15 with a reward of 1 is equal to 0.33333 if going right towards the goal.\n",
    "\n",
    "Going backwards is not possible when trying to walk in a certain direction, but going left or right instead is possible.\n",
    "\n",
    "```\n",
    "env.P[7][ACTION_RIGHT] = \n",
    "[(1.0, 7, 0, True)]\n",
    "```\n",
    "Which means that any action taken from a hole still ends up in the hole.\n",
    "\n",
    "Transitions at the edge can end up in the same state for some actions, while transitions in the middle of the board (except holes) always end up in another state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1734a",
   "metadata": {},
   "source": [
    "## Value iteration without policy updates\n",
    "To check if this approach works. Uses a static policy of \"always go right\", which turns out to have a 3% success rate from state 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration_version1(mdp, gamma, num_iterations):\n",
    "    V = np.zeros(mdp.num_states) # maps state -> value\n",
    "    policy = 2*np.ones(mdp.num_states) # maps state -> action\n",
    "    \n",
    "    for it in range(num_iterations):\n",
    "        Vprev = V\n",
    "        V = np.zeros(mdp.num_states)\n",
    "        \n",
    "        for s in range(mdp.num_states):\n",
    "            V[s] = 0.0\n",
    "            action = policy[s]\n",
    "            for sPrime in range(mdp.num_states):\n",
    "                probability = mdp.probability(s, action, sPrime)\n",
    "                reward = mdp.reward(s, action, sPrime)\n",
    "                V[s] += probability * (reward + Vprev[sPrime])\n",
    "                \n",
    "    return V\n",
    "        \n",
    "value_iteration_version1(mdp, 0.9, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94949f3",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d67439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration_version2(mdp, gamma, num_iterations):\n",
    "    V = np.zeros(mdp.num_states) # maps state -> value\n",
    "    policy = np.zeros(mdp.num_states, dtype=int) # maps state -> action\n",
    "    \n",
    "    for it in range(num_iterations):\n",
    "        Vprev = V\n",
    "        policy_prev = policy\n",
    "        \n",
    "        V = np.zeros(mdp.num_states)\n",
    "        policy = np.zeros(mdp.num_states, dtype=int)\n",
    "        \n",
    "        # Evaluate current policy\n",
    "        for s in range(mdp.num_states):\n",
    "            V[s] = 0.0\n",
    "            action = policy_prev[s]\n",
    "            for sPrime in range(mdp.num_states):\n",
    "                probability = mdp.probability(s, action, sPrime)\n",
    "                reward = mdp.reward(s, action, sPrime)\n",
    "                V[s] += probability * (reward + Vprev[sPrime])\n",
    "                \n",
    "        # Improve policy\n",
    "        for s in range(mdp.num_states):\n",
    "            expected_value_for_action = np.zeros(mdp.num_actions)\n",
    "            for action in range(mdp.num_actions):\n",
    "                sum_over_sPrime = 0.0\n",
    "                for sPrime in range(mdp.num_states):\n",
    "                    probability = mdp.probability(s, action, sPrime)\n",
    "                    reward = mdp.reward(s, action, sPrime)\n",
    "                    sum_over_sPrime += probability * (reward + Vprev[sPrime])\n",
    "                expected_value_for_action[action] = sum_over_sPrime\n",
    "            policy[s] = np.argmax(expected_value_for_action)\n",
    "\n",
    "        max_diff = np.abs(V - Vprev).max()\n",
    "        #print(max_diff)\n",
    "        #print(\"win chance: \", V[0])\n",
    "                \n",
    "    return V, policy\n",
    "        \n",
    "V, policy = value_iteration_version2(mdp, 0.95, 1000)\n",
    "print(\"win chance: \", V[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570c603",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def simulate_episode(policy):\n",
    "    env.reset()\n",
    "    observation = 0\n",
    "    for t in range(100):\n",
    "        render_inline(env)\n",
    "        action = policy[observation]\n",
    "        print(f\"Going {action_name[action]}\")\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated:\n",
    "            break\n",
    "    assert terminated\n",
    "    render_inline(env)\n",
    "\n",
    "simulate_episode(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b121e",
   "metadata": {},
   "source": [
    "It works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043317a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
